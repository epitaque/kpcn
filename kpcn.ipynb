{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Predicting Convolutional Networks for Denoising Monte Carlo Renderings\n",
    "*Original Paper: http://civc.ucsb.edu/graphics/Papers/SIGGRAPH2017_KPCN/PaperData/SIGGRAPH17_KPCN.pdf*  \n",
    "*TensorFlow Implementation by **Brian Hoy***  \n",
    "*Training data by **Matthew Woerner***  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.feature_extraction import image\n",
    "import numpy as np\n",
    "import imageio\n",
    "import math\n",
    "import IPython.display\n",
    "import PIL\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "K.set_floatx('float32')\n",
    "\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for learning\n",
    "mode = \"kpcn\" # \"kpcn\" or \"dpcn\"\n",
    "patch_size = 64\n",
    "mini_batch_size = 4 if mode == \"kpcn\" else 5 # KPCN requires much more video memory so we have to reduce batch size :(\n",
    "percent_used_for_validation = 0.1\n",
    "learning_rate = 10e-6\n",
    "output_kernel_size = 17\n",
    "enable_albedo_div = True\n",
    "checkpoint_path = \"training_\" + mode + (\"_albedodiv\" if enable_albedo_div else \"\") + \"/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "restore_from_checkpoint = False\n",
    "do_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_3_channels(image):\n",
    "    if image.ndim == 2: image = np.dstack((image, image, image))\n",
    "    if image.shape[2] == 1: \n",
    "        image = np.reshape(image, (image.shape[0], image.shape[1]))\n",
    "        image = np.dstack((image, image, image))\n",
    "    image = image[:,:,:3]\n",
    "    return image\n",
    "\n",
    "def preprocess_variance(variance):\n",
    "    variance = variance.max(axis=2)\n",
    "#     variance = np.minimum(variance**(1.0/2.2), 1.0)\n",
    "#     variance = variance / variance.max()\n",
    "    return variance\n",
    "    \n",
    "def preprocess_color(color, albedo, epsilon=0.1):\n",
    "    return color.astype(np.float32) / (albedo + epsilon)\n",
    "\n",
    "def postprocess_color(color, albedo, epsilon=0.1):\n",
    "    return color * (albedo + epsilon)\n",
    "\n",
    "def preprocess_color_variance(variance, albedo, epsilon=0.1):\n",
    "    variance = variance / (albedo + epsilon)**2\n",
    "    return preprocess_variance(variance)\n",
    "    \n",
    "def calculate_gradient(image):\n",
    "    y, x, c = image.shape\n",
    "    dx = image[:, 1:, :] - image[:, :x-1, :]\n",
    "    dy = image[1:, :, :] - image[:y-1, :, :]\n",
    "    dx = np.append(dx, np.zeros([y, 1, c]), axis=1)\n",
    "    dy = np.append(dy, np.zeros([1, x, c]), axis=0)\n",
    "    grad = np.dstack((dx, dy))\n",
    "    return grad\n",
    "\n",
    "def load_highspp(folder_path):\n",
    "    albedo = make_3_channels(imageio.imread(folder_path + \"/albedo.png\").astype(np.float32))\n",
    "    color = make_3_channels(imageio.imread(folder_path + \"/final.png\").astype(np.float32))\n",
    "    if enable_albedo_div:\n",
    "        color = preprocess_color(color, albedo)\n",
    "    \n",
    "    combined = np.dstack((color, albedo))\n",
    "    combined = combined.astype(np.float16)\n",
    "    assert not np.any(np.isnan(combined))\n",
    "    return combined\n",
    "\n",
    "def load_lowspp(folder_path, return_channels=False):\n",
    "    print(\"loading \" + folder_path)\n",
    "\n",
    "    # albedo buffers\n",
    "    albedo = make_3_channels(imageio.imread(folder_path + \"/albedo.png\").astype(np.float32))\n",
    "    albedo_variance = make_3_channels(imageio.imread(folder_path + \"/albedoVariance.png\").astype(np.float32))\n",
    "    albedo_variance = preprocess_variance(albedo_variance)\n",
    "    albedo_gradient = calculate_gradient(albedo)\n",
    "\n",
    "    # color buffers\n",
    "    color = make_3_channels(imageio.imread(folder_path + \"/final.png\").astype(np.float32))\n",
    "    if enable_albedo_div:\n",
    "        color = preprocess_color(color, albedo)\n",
    "    color_variance = make_3_channels(imageio.imread(folder_path + \"/colorVariance.png\").astype(np.float32))\n",
    "    if enable_albedo_div:\n",
    "        color_variance = preprocess_color_variance(color_variance, albedo)\n",
    "    else:\n",
    "        color_variance = preprocess_variance(color_variance)\n",
    "\n",
    "    color_gradient = calculate_gradient(color)\n",
    "\n",
    "    # depth buffers\n",
    "    depth = imageio.imread(folder_path + \"/depth.png\").astype(np.float32)\n",
    "    depth = depth.reshape((depth.shape[0], depth.shape[1], 1))\n",
    "    depth_variance = make_3_channels(imageio.imread(folder_path + \"/depthVariance.png\").astype(np.float32))\n",
    "    depth_variance = preprocess_variance(depth_variance)\n",
    "    # depth_gradient = calculate_gradient(depth)\n",
    "\n",
    "    # normal buffers\n",
    "    normal = make_3_channels(imageio.imread(folder_path + \"/normal.png\").astype(np.float32))\n",
    "    normal_variance = make_3_channels(imageio.imread(folder_path + \"/normalVariance.png\").astype(np.float32))\n",
    "    normal_variance = preprocess_variance(normal_variance)\n",
    "    # normal_gradient = calculate_gradient(normal)\n",
    "    \n",
    "    normal_variance = normal_variance.reshape((normal_variance.shape[0], normal_variance.shape[1], 1))\n",
    "    color_variance = color_variance.reshape((color_variance.shape[0], color_variance.shape[1], 1))\n",
    "    albedo_variance = albedo_variance.reshape((albedo_variance.shape[0], albedo_variance.shape[1], 1))\n",
    "    depth_variance = depth_variance.reshape((depth_variance.shape[0], depth_variance.shape[1], 1))\n",
    "\n",
    "    chans = (color, albedo, color_gradient, albedo_gradient, depth, normal, color_variance, albedo_variance, depth_variance, normal_variance)\n",
    "    chans_str = \"color, albedo, color_gradient, albedo_gradient, depth, normal, color_variance, albedo_variance, depth_variance, normal_variance\".split(\", \")\n",
    "    names_and_channels = []\n",
    "    if return_channels:\n",
    "        for i in range(len((chans))):\n",
    "            names_and_channels.append((chans_str[i], chans[i].shape[2]))\n",
    "    \n",
    "    combined = np.dstack((color, albedo, color_gradient, albedo_gradient, depth, normal, color_variance, albedo_variance, depth_variance, normal_variance))\n",
    "    combined = combined.astype(np.float16)\n",
    "    assert not np.any(np.isnan(combined))\n",
    "    \n",
    "    if return_channels:\n",
    "        return combined, names_and_channels\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(data, display_image=True):\n",
    "    data = np.clip(data, 0.0, 255.0).astype(np.uint8)\n",
    "    pil_img = PIL.Image.fromarray(data, 'RGB')\n",
    "    if display_image:\n",
    "        display(pil_img)\n",
    "    return pil_img\n",
    "\n",
    "def imshow_pp(data, display_image=True):\n",
    "    if enable_albedo_div:\n",
    "        data = postprocess_color(data[:,:,:3], data[:,:,3:])\n",
    "    return imshow(data[:,:,:3], display_image=display_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(image):\n",
    "    half_patch_size = patch_size//2\n",
    "    x_patches = image.shape[1]//patch_size\n",
    "    x_patches = x_patches + x_patches - 1\n",
    "    y_patches = image.shape[0]//patch_size\n",
    "    y_patches = y_patches + y_patches - 1\n",
    "    add_extra_x_patch = (image.shape[1] % patch_size) != 0\n",
    "    add_extra_y_patch = (image.shape[0] % patch_size) != 0\n",
    "    patches = []\n",
    "    \n",
    "    xstart = 0\n",
    "    xend = 0\n",
    "    for x in range(x_patches):\n",
    "        xstart = x * half_patch_size\n",
    "        xend = xstart + patch_size\n",
    "        for y in range(y_patches):\n",
    "            ystart = y * half_patch_size\n",
    "            yend = ystart + patch_size\n",
    "            patches.append(image[ystart:yend, xstart:xend, :])\n",
    "            \n",
    "        # add one more vertical patch if needed\n",
    "        if add_extra_y_patch:\n",
    "            patches.append(image[image.shape[0]-patch_size:, xstart:xend, :])\n",
    "    \n",
    "    if add_extra_x_patch:\n",
    "        for y in range(y_patches):\n",
    "            ystart = y * half_patch_size\n",
    "            yend = ystart + patch_size\n",
    "            temp = image[ystart:yend, image.shape[1]-patch_size:, :]\n",
    "            patches.append(temp)\n",
    "        if add_extra_y_patch:\n",
    "            patches.append(image[image.shape[0]-patch_size:, xstart:xend, :])\n",
    "\n",
    "    return patches\n",
    "\n",
    "def reassemble_patches(patches, image_shape):\n",
    "    half_patch_size = patch_size//2\n",
    "    x_patches = image_shape[1]//patch_size\n",
    "    x_patches = x_patches + x_patches - 1\n",
    "    y_patches = image_shape[0]//patch_size\n",
    "    y_patches = y_patches + y_patches - 1\n",
    "    add_extra_x_patch = (image_shape[1] % patch_size) != 0\n",
    "    add_extra_y_patch = (image_shape[0] % patch_size) != 0\n",
    "    \n",
    "    xstart = 0\n",
    "    xend = 0\n",
    "    ystart = 0\n",
    "    yend = 0\n",
    "    i = 0\n",
    "    image = None\n",
    "    for x in range(x_patches):\n",
    "        xstart = x * half_patch_size\n",
    "        xend = xstart + patch_size\n",
    "        vertstrip = patches[i]\n",
    "        i = i + 1\n",
    "        for y in range(y_patches - 1):\n",
    "            ystart = y * half_patch_size\n",
    "            yend = ystart + patch_size\n",
    "            vertstrip = np.vstack((vertstrip, patches[i]))\n",
    "            i = i + 1\n",
    "            \n",
    "        # add one more vertical patch if needed\n",
    "        if add_extra_y_patch:\n",
    "            vertstrip = np.vstack((vertstrip, patches[i][16:, :, :]))\n",
    "            i = i + 1\n",
    "\n",
    "        image = vertstrip if type(image) == type(None) else np.hstack((image, vertstrip))\n",
    "    \n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note to self: use this https://www.tensorflow.org/api_docs/python/tf/data/experimental/sample_from_datasets\n",
    "def load_x_y_patches():\n",
    "    x_patches = []\n",
    "    y_patches = []\n",
    "    y_patches_with_albedo = []\n",
    "    orig = next(os.walk('./pngs'))[1]\n",
    "    folders = [x[0:x.rindex('16')] for x in orig if '16' in x]\n",
    "\n",
    "    for x in folders:\n",
    "        low_spp = load_lowspp('./pngs/' + x + '16')\n",
    "        high_spp = load_highspp('./pngs/' + x + '4096')\n",
    "        temp_x_patches = extract_patches(low_spp)\n",
    "        temp_y_patches = extract_patches(high_spp)\n",
    "\n",
    "        for i in range(len(temp_x_patches)):\n",
    "            x_patches.append(temp_x_patches[i])\n",
    "            y_patches.append(temp_y_patches[i][16:-16,16:-16,:3])\n",
    "            y_patches_with_albedo.append(temp_y_patches[i])\n",
    "\n",
    "    return x_patches, y_patches, y_patches_with_albedo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_patches, y_patches, y_patches_with_albedo = load_x_y_patches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_input_channels(patch_number=360):\n",
    "    print(\"x patch shape: \" + str(x_patches[0].shape))\n",
    "    img, names_and_channels = load_lowspp(\"./pngs/bathroom16\", return_channels=True)\n",
    "    patch = x_patches[patch_number]\n",
    "    \n",
    "    if enable_albedo_div:\n",
    "        print(\"color div albedo\")\n",
    "        imshow(patch[:,:,:3])\n",
    "        print(\"color\")\n",
    "        imshow_pp(patch[:,:,:6])\n",
    "    else:\n",
    "        print(\"color\")\n",
    "        imshow(patch[:,:,:3])\n",
    "    \n",
    "    i = 3\n",
    "    for name, channels in names_and_channels[1:]:\n",
    "        if channels == 6:\n",
    "            channels = 3\n",
    "            print(name + \" x\")\n",
    "            imshow(make_3_channels(img[:,:,i:(i+channels)]))\n",
    "            i = i + channels\n",
    "            print(name + \" y\")\n",
    "            imshow(make_3_channels(img[:,:,i:(i+channels)]))\n",
    "            i = i + channels\n",
    "\n",
    "        else:\n",
    "            print(name)\n",
    "            imshow(make_3_channels(img[:,:,i:(i+channels)]))\n",
    "            i = i + channels\n",
    "        \n",
    "#     print(\"y label: shape: \" + str(y_patches[0].shape))\n",
    "#     imshow(y_patches[patch_number][:,:,:])\n",
    "\n",
    "# preview_input_channels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Keras layer to apply the kernel to noisy input\n",
    "# Works by shifting the input then multiplying it by the appropriate weight\n",
    "# Note that all weights will be between 0 and 1 due to the softmax activation of previous layer\n",
    "\n",
    "halfsize = output_kernel_size//2\n",
    "\n",
    "class KernelApply2D(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(KernelApply2D, self).__init__()\n",
    "\n",
    "    def call(self, input):\n",
    "        w = input[:,16:-16,16:-16,3:]\n",
    "        w = tf.stack([w, w, w], axis=3)\n",
    "        y = input[:,:,:,:3]\n",
    "        z = input[:,16:-16,16:-16,:3]*0\n",
    "\n",
    "        for i in range(output_kernel_size):\n",
    "            for j in range(output_kernel_size):\n",
    "                shifted_input = y[:,16-halfsize+j:-16-halfsize+j,16-halfsize+i:-16-halfsize+i]\n",
    "                z = z + shifted_input * w[:,:,:,:,i*3 + j]\n",
    "\n",
    "        return z\n",
    "\n",
    "# Tests kernel apply layer by applying a simple edge detection kernel\n",
    "def test_kernel_apply_2D():\n",
    "    global output_kernel_size, halfsize\n",
    "    old_output_kernel_size = output_kernel_size\n",
    "    output_kernel_size = 3\n",
    "    halfsize = output_kernel_size//2\n",
    "    layer = KernelApply2D()\n",
    "    # xpatch = y_patches[405][:,:,:3]\n",
    "    # print(xpatch.dtype)\n",
    "    xpatch = tf.ones((32,32,3), dtype=tf.float16) * 255.0\n",
    "    xpatch = tf.pad(xpatch, [[16, 16], [16, 16], [0, 0]], \"CONSTANT\")\n",
    "    x = xpatch[:,:,0]\n",
    "    x = tf.repeat(x, 3)\n",
    "\n",
    "    x = tf.reshape(x, (1, 64, 64, 3))\n",
    "    w = tf.constant([1.0, 2.0, 1.0, 0.0, 0.0, 0.0, -1.0, -2.0, -1.0], dtype=tf.float16)\n",
    "    w = tf.reshape(w, (1, 1, 1, 9))\n",
    "    w = tf.tile(w, tf.constant([1, 64, 64, 1]))\n",
    "\n",
    "    x = tf.concat([x, w], axis=3)\n",
    "    predicted = layer(x)[0]\n",
    "\n",
    "    imshow(xpatch)\n",
    "    z = imshow(predicted)\n",
    "    output = layer(x)\n",
    "    output[0,0,0]\n",
    "    output_kernel_size = old_output_kernel_size\n",
    "    halfsize = output_kernel_size//2\n",
    "\n",
    "# test_kernel_apply_2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I could not find a built in keras layer to crop the channels so I made a really simple one... used for extracting color channels from input\n",
    "class ChannelCroppingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ChannelCroppingLayer, self).__init__()\n",
    "\n",
    "    def call(self, input):\n",
    "        return input[:,:,:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nn(input_channels):\n",
    "    #basic model parameters\n",
    "    L = 9\n",
    "    kernel_size = 5\n",
    "    hidden_channels = 100\n",
    "    in_between_layers = L - 2\n",
    "\n",
    "    # add layers\n",
    "    noisy_input = keras.Input(shape=(patch_size, patch_size, input_channels))\n",
    "    x = noisy_input\n",
    "    for i in range(in_between_layers):\n",
    "#         x = layers.ZeroPadding2D(padding=(2, 2))(x)\n",
    "        x = layers.Conv2D(hidden_channels, kernel_size=kernel_size, kernel_initializer=\"glorot_uniform\", strides=1, activation=\"relu\")(x)\n",
    "\n",
    "#     x = layers.ZeroPadding2D(padding=(2, 2))(x)\n",
    "    \n",
    "    if mode == \"kpcn\":\n",
    "        cropped_noisy_input = ChannelCroppingLayer()(noisy_input)\n",
    "        x = layers.Conv2D(output_kernel_size**2, kernel_size=kernel_size, kernel_initializer=\"glorot_uniform\", strides=1, activation=\"softmax\")(x)\n",
    "        x = layers.ZeroPadding2D(padding=(16, 16))(x)\n",
    "        x = layers.concatenate([cropped_noisy_input, x])\n",
    "        x = KernelApply2D()(x)\n",
    "    else:\n",
    "        x = layers.Conv2D(3, kernel_size=kernel_size, kernel_initializer=\"glorot_uniform\", strides=1, activation=\"relu\")(x)\n",
    "    \n",
    "    # compile model\n",
    "    model = keras.Model(inputs=[noisy_input], outputs=[x])\n",
    "    opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"mean_absolute_error\", optimizer=opt, metrics=['mean_absolute_error'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = make_nn(x_patches[0].shape[2])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(x_patches, y_patches):\n",
    "    validate_amount = math.floor(len(x_patches) * percent_used_for_validation)\n",
    "    rec_count = len(x_patches)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x_patches, y_patches))\n",
    "    ds = ds.shuffle(buffer_size=400, reshuffle_each_iteration=True)\n",
    "    train = ds.skip(validate_amount).batch(mini_batch_size)\n",
    "    validate = ds.take(validate_amount).batch(mini_batch_size)\n",
    "    return train, validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_training:\n",
    "    train, validate = create_tf_dataset(x_patches, y_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restore_from_checkpoint:\n",
    "    latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    model.load_weights(latest)\n",
    "    model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(lowspp_imagedir, highspp_imagedir, display_image=False):\n",
    "    lowspp = load_lowspp(lowspp_imagedir)\n",
    "    highspp = load_highspp(highspp_imagedir)\n",
    "    x_patches = extract_patches(lowspp)\n",
    "    y_patches = extract_patches(highspp)\n",
    "    pred_patches = []\n",
    "    for i in range(len(x_patches)):\n",
    "        patch_x = x_patches[i]\n",
    "        patch_y = y_patches[i]\n",
    "        predicted = model.predict(patch_x.reshape((1, patch_size, patch_size, patch_x.shape[2])))[0]\n",
    "        if enable_albedo_div:\n",
    "            predicted = np.dstack((predicted, patch_y[16:-16,16:-16,3:]))\n",
    "        pred_patches.append(predicted)\n",
    "    image = reassemble_patches(pred_patches, lowspp.shape)\n",
    "    return imshow_pp(image, display_image=display_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_and_save_image(patch_number=360, epoch=-1):\n",
    "    xpatch = x_patches[patch_number]\n",
    "    xpatchcolor = xpatch[:,:,:6]\n",
    "    gt = y_patches_with_albedo[patch_number]\n",
    "    predicted = model.predict(xpatch.reshape((1, xpatch.shape[0], xpatch.shape[1], xpatch.shape[2])))[0]\n",
    "    if enable_albedo_div:\n",
    "        predicted = np.dstack((predicted, gt[16:-16,16:-16,3:]))\n",
    "        imshow_pp(xpatchcolor).save(checkpoint_dir + '/epoch_' + str(epoch) + '_xpatch.png')\n",
    "        imshow_pp(gt).save(checkpoint_dir + '/epoch_' + str(epoch) + '_gt.png')\n",
    "        imshow(gt[:,:,:3]).save(checkpoint_dir + '/epoch_' + str(epoch) + '_gt_divalbedo.png')\n",
    "        imshow_pp(predicted).save(checkpoint_dir + '/epoch_' + str(epoch) + '_prediction.png')\n",
    "        imshow(predicted[:,:,:3]).save(checkpoint_dir + '/epoch_' + str(epoch) + '_prediction_divalbedo.png')\n",
    "    else:\n",
    "        imshow(xpatchcolor[:,:,:3]).save(checkpoint_dir + '/epoch_' + str(epoch) + '_xpatch.png')\n",
    "        imshow(gt[:,:,:3]).save(checkpoint_dir + '/epoch_' + str(epoch) + '_gt.png')\n",
    "        imshow_pp(predicted[:,:,:3]).save(checkpoint_dir + '/epoch_' + str(epoch) + '_prediction.png')\n",
    "\n",
    "class DisplayImageCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(DisplayImageCallback, self).__init__()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        display_and_save_image(epoch=epoch)\n",
    "#         if epoch % 3 == 0:\n",
    "        epoch = epoch + 1 + 3\n",
    "        test_model('./pngs/bathroom16', './pngs/bathroom4096', display_image=False).save(checkpoint_dir + '/bathroom/epoch_' + str(epoch) + '_predicted.png')\n",
    "\n",
    "# print(\"Model without any training\")\n",
    "# display_and_save_image()\n",
    "# test_model('./pngs/bathroom16', './pngs/bathroom4096', display_image=True).save(checkpoint_dir + '/bathroom/epoch_' + str(-1) + '_predicted_full_bathroom.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fit model on training data\")\n",
    "image_cb = DisplayImageCallback()\n",
    "history_cb = tf.keras.callbacks.CSVLogger(checkpoint_dir + '/log.csv', separator=\",\", append=True)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "if do_training:\n",
    "    history = model.fit(\n",
    "        train,\n",
    "        validation_data=validate,\n",
    "        epochs=10000,\n",
    "        callbacks=[cp_callback, history_cb, image_cb]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(x_patches, y_patches, lowsppshape):\n",
    "    pred_patches = []\n",
    "    for i in range(len(x_patches)):\n",
    "        patch_x = x_patches[i]\n",
    "        patch_y = y_patches[i]\n",
    "        predicted = model.predict(patch_x.reshape((1, patch_size, patch_size, patch_x.shape[2])))[0]\n",
    "        pred_patches.append(predicted)\n",
    "    image = reassemble_patches(pred_patches, lowsppshape)\n",
    "    return imshow_pp(image, display_image=False)\n",
    "\n",
    "def test_all():\n",
    "    x_patches = []\n",
    "    y_patches = []\n",
    "    y_patches_with_albedo = []\n",
    "    orig = next(os.walk('./pngs-applying-denoiser-to'))[1]\n",
    "    folders = [x[0:x.rindex('16')] for x in orig if '16' in x]\n",
    "    folders = folders + [x[0:x.rindex('32')] for x in orig if '32' in x]\n",
    "    folders = list(set(folders))\n",
    "    \n",
    "    for x in folders:\n",
    "        try:\n",
    "            low_spp_path = './pngs-applying-denoiser-to/' + x + '16'\n",
    "            med_spp_path = './pngs-applying-denoiser-to/' + x + '32'\n",
    "            high_spp_path = './pngs-applying-denoiser-to/' + x + '4096'\n",
    "            output_path = './output/' + x\n",
    "\n",
    "            if not os.path.exists(output_path):\n",
    "                os.makedirs(output_path)\n",
    "\n",
    "            high_spp = load_highspp(high_spp_path)\n",
    "            y_patches = extract_patches(high_spp)\n",
    "\n",
    "            if os.path.isdir(low_spp_path):\n",
    "                low_spp = load_lowspp('./pngs-applying-denoiser-to/' + x + '16')\n",
    "                x_patches = extract_patches(low_spp)\n",
    "                image = denoise(x_patches, y_patches, low_spp.shape)\n",
    "                image.save(output_path + '/' + mode + '_denoised_16spp.png')\n",
    "                x_patches = [x[16:-16, 16:-16, :] for x in x_patches]\n",
    "                imshow_pp(reassemble_patches(x_patches, low_spp.shape), display_image=False).save(output_path + '/orig_16spp.png')\n",
    "\n",
    "            if os.path.isdir(med_spp_path):\n",
    "                low_spp = load_lowspp('./pngs-applying-denoiser-to/' + x + '32')\n",
    "                x_patches = extract_patches(low_spp)\n",
    "                image = denoise(x_patches, y_patches, low_spp.shape)\n",
    "                image.save(output_path + '/' + mode + '_denoised_32spp.png')\n",
    "                x_patches = [x[16:-16, 16:-16, :] for x in x_patches]\n",
    "                imshow_pp(reassemble_patches(x_patches, low_spp.shape), display_image=False).save(output_path + '/orig_32spp.png')\n",
    "\n",
    "            y_patches = [x[16:-16, 16:-16, :] for x in y_patches]\n",
    "            imshow_pp(reassemble_patches(y_patches, low_spp.shape), display_image=False).save(output_path + '/ground_truth.png')\n",
    "        except e:\n",
    "            print(\"Unable to denoise folder \" + x + \". Exception: \" + str(e))\n",
    "        \n",
    "test_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "To make this I used:\n",
    "1) the TensorFlow/Keras Documentation\n",
    "2) the KPCN paper (http://civc.ucsb.edu/graphics/Papers/SIGGRAPH2017_KPCN/PaperData/SIGGRAPH17_KPCN.pdf)\n",
    "3) a PyTorch KPCN implementation I found on Google (https://github.com/Nidjo123/kpcn) to validate that structure of my CNN is correct as well as see how they preprocessed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowspp = load_lowspp('./pngs/coffee-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patches = extract_patches(lowspp)\n",
    "# patches = [x[16:-16, 16:-16, :] for x in patches]\n",
    "# print(patches[0].shape)\n",
    "# len(patches)\n",
    "# rebuilt = reassemble_patches(patches, lowspp.shape)\n",
    "# imshow(rebuilt[:,:,:3])\n",
    "# print(rebuilt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model('./pngs/bathroom16', './pngs/bathroom4096').save(checkpoint_dir + '/epoch_' + str(-2) + '_predicted_full_bathroom.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "2cd2abeaea65c445b1f865368dbbfc01b247d4af0d8b7b9288cea8582e68bfb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
